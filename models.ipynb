{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#for modeling\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Dropout\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "#for concept drift detection\n",
    "from skmultiflow.drift_detection.adwin import ADWIN\n",
    "from skmultiflow.drift_detection import DDM\n",
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from skmultiflow.drift_detection.hddm_a import HDDM_A\n",
    "from skmultiflow.drift_detection.hddm_w import HDDM_W\n",
    "from skmultiflow.drift_detection import KSWIN\n",
    "from skmultiflow.drift_detection import PageHinkley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the data has been combined for all years 1999-2018, and split in the get_data_and_processing notebook. I still need to determine how to structure this data and present in a sequential order. \n",
    "\n",
    "+ Do I setup models for 1999-2001 and then structure the data for the remaining years and train and test incrementally?\n",
    "  + If I go this route, do I use the full data set ordered by year with no need to do cross validation or train_test_split since the testing will be done sequentially?\n",
    "+ In this current form, since all years are shuffled together, the below drift detection is picking up some drifts, especially for the ADWIN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/X_train.csv', index_col=0)\n",
    "y_train = pd.read_csv('data/y_train.csv', index_col=0)\n",
    "X_test = pd.read_csv('data/X_test.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/y_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models to Run:\n",
    "\n",
    "+ Logistic Regression\n",
    "+ Support Vector Machines\n",
    "+ Random Forest\n",
    "+ XGBoost\n",
    "+ Neural Networks\n",
    "\n",
    "Check if there are others that should be run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>LDL Chol</th>\n",
       "      <th>HDL Chol</th>\n",
       "      <th>Chloride</th>\n",
       "      <th>Total Chol</th>\n",
       "      <th>GGT</th>\n",
       "      <th>AST</th>\n",
       "      <th>Hemoglobin</th>\n",
       "      <th>Weight</th>\n",
       "      <th>BMI</th>\n",
       "      <th>ALP</th>\n",
       "      <th>ALT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>77396.000000</td>\n",
       "      <td>77396.000000</td>\n",
       "      <td>77396.000000</td>\n",
       "      <td>77396.000000</td>\n",
       "      <td>77396.000000</td>\n",
       "      <td>77396.000000</td>\n",
       "      <td>77396.000000</td>\n",
       "      <td>77396.000000</td>\n",
       "      <td>77396.00000</td>\n",
       "      <td>77396.000000</td>\n",
       "      <td>77396.000000</td>\n",
       "      <td>77396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.580637</td>\n",
       "      <td>108.698611</td>\n",
       "      <td>53.270547</td>\n",
       "      <td>103.346497</td>\n",
       "      <td>183.948457</td>\n",
       "      <td>26.755135</td>\n",
       "      <td>24.945263</td>\n",
       "      <td>13.810239</td>\n",
       "      <td>64.07973</td>\n",
       "      <td>25.471295</td>\n",
       "      <td>87.849703</td>\n",
       "      <td>23.830325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>24.586185</td>\n",
       "      <td>20.226025</td>\n",
       "      <td>11.142197</td>\n",
       "      <td>2.381670</td>\n",
       "      <td>37.068970</td>\n",
       "      <td>33.109661</td>\n",
       "      <td>13.970365</td>\n",
       "      <td>1.394973</td>\n",
       "      <td>29.22481</td>\n",
       "      <td>7.251190</td>\n",
       "      <td>46.227507</td>\n",
       "      <td>17.867682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>6.40000</td>\n",
       "      <td>11.490000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>108.710285</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>12.900000</td>\n",
       "      <td>47.40000</td>\n",
       "      <td>20.300000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>108.710285</td>\n",
       "      <td>53.277988</td>\n",
       "      <td>103.345424</td>\n",
       "      <td>183.944825</td>\n",
       "      <td>26.740553</td>\n",
       "      <td>24.945995</td>\n",
       "      <td>13.806845</td>\n",
       "      <td>64.20000</td>\n",
       "      <td>25.494282</td>\n",
       "      <td>87.745839</td>\n",
       "      <td>23.851419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>53.000000</td>\n",
       "      <td>108.710285</td>\n",
       "      <td>53.277988</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>26.740553</td>\n",
       "      <td>24.945995</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>82.20000</td>\n",
       "      <td>29.200000</td>\n",
       "      <td>87.745839</td>\n",
       "      <td>23.851419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>629.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>813.000000</td>\n",
       "      <td>2274.000000</td>\n",
       "      <td>1672.000000</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>371.00000</td>\n",
       "      <td>130.210000</td>\n",
       "      <td>1378.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Age      LDL Chol      HDL Chol      Chloride    Total Chol  \\\n",
       "count  77396.000000  77396.000000  77396.000000  77396.000000  77396.000000   \n",
       "mean      32.580637    108.698611     53.270547    103.346497    183.948457   \n",
       "std       24.586185     20.226025     11.142197      2.381670     37.068970   \n",
       "min        1.000000      9.000000      6.000000     70.000000     59.000000   \n",
       "25%       11.000000    108.710285     50.000000    103.000000    161.000000   \n",
       "50%       26.000000    108.710285     53.277988    103.345424    183.944825   \n",
       "75%       53.000000    108.710285     53.277988    104.000000    199.000000   \n",
       "max       85.000000    629.000000    226.000000    120.000000    813.000000   \n",
       "\n",
       "                GGT           AST    Hemoglobin       Weight           BMI  \\\n",
       "count  77396.000000  77396.000000  77396.000000  77396.00000  77396.000000   \n",
       "mean      26.755135     24.945263     13.810239     64.07973     25.471295   \n",
       "std       33.109661     13.970365      1.394973     29.22481      7.251190   \n",
       "min        2.000000      6.000000      5.900000      6.40000     11.490000   \n",
       "25%       15.000000     21.000000     12.900000     47.40000     20.300000   \n",
       "50%       26.740553     24.945995     13.806845     64.20000     25.494282   \n",
       "75%       26.740553     24.945995     14.600000     82.20000     29.200000   \n",
       "max     2274.000000   1672.000000     19.900000    371.00000    130.210000   \n",
       "\n",
       "                ALP           ALT  \n",
       "count  77396.000000  77396.000000  \n",
       "mean      87.849703     23.830325  \n",
       "std       46.227507     17.867682  \n",
       "min        7.000000      2.000000  \n",
       "25%       68.000000     17.000000  \n",
       "50%       87.745839     23.851419  \n",
       "75%       87.745839     23.851419  \n",
       "max     1378.000000   1997.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape y_train to be a column vector\n",
    "y_train = y_train.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('logistic',\n",
       "                                        LogisticRegression(max_iter=1000))]),\n",
       "             param_grid={'logistic__C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "logistic = LogisticRegression(max_iter=1000)\n",
    "pipe = Pipeline([('scaler', scaler),('logistic', logistic)])\n",
    "param_grid = {'logistic__C': np.logspace(-3, 3, 7)}\n",
    "logistic_model = GridSearchCV(pipe, param_grid, cv=5)\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logistic__C': 0.01}\n",
      "0.9187167184747927\n",
      "Train set MSE performance: 0.0813\n",
      "Test set MSE performance: 0.0822\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression \n",
    "print(logistic_model.best_params_)\n",
    "print(logistic_model.best_score_)\n",
    "\n",
    "#performance\n",
    "print('Train set MSE performance: ' + str(round(mean_squared_error(y_train,logistic_model.predict(X_train)),4))) #Train set prediction and performance\n",
    "print('Test set MSE performance: ' + str(round(mean_squared_error(y_test,logistic_model.predict(X_test)),4))) #Test set prediction and performance\n",
    "\n",
    "#Predictions\n",
    "logistic_y_pred = logistic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:292: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('svm', SVC(max_iter=1000))]),\n",
       "             param_grid={'svm__C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03])})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Support Vector Machines\n",
    "svm = SVC(max_iter=1000)\n",
    "pipe = Pipeline([('scaler', scaler),('svm', svm)])\n",
    "param_grid = {'svm__C': np.logspace(-3, 3, 7)}\n",
    "svm_model = GridSearchCV(pipe, param_grid, cv=5)\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svm__C': 0.001}\n",
      "0.7503106584514854\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('svm', SVC(C=0.001, max_iter=1000))])\n",
      "Train set performance: 0.3315\n",
      "Test set performance: 0.3331\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machines\n",
    "print(svm_model.best_params_)\n",
    "print(svm_model.best_score_)\n",
    "print(svm_model.best_estimator_)\n",
    "\n",
    "#performance\n",
    "print('Train set performance: ' + str(round(mean_squared_error(y_train,svm_model.predict(X_train)),4))) #Train set prediction and performance\n",
    "print('Test set performance: ' + str(round(mean_squared_error(y_test,svm_model.predict(X_test)),4))) #Test set prediction and performance\n",
    "\n",
    "#Predictions\n",
    "svm_y_pred = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('rf',\n",
       "                                        RandomForestClassifier(max_depth=5,\n",
       "                                                               random_state=0))]),\n",
       "             param_grid={'rf__max_depth': [3, 5, 7],\n",
       "                         'rf__n_estimators': [100, 200, 300]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "pipe = Pipeline([('scaler', scaler),('rf', rf)])\n",
    "param_grid = {'rf__max_depth': [3, 5, 7], 'rf__n_estimators': [100, 200, 300]}\n",
    "rf_model = GridSearchCV(pipe, param_grid, cv=5)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf__max_depth': 7, 'rf__n_estimators': 100}\n",
      "0.9190268193650045\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('rf', RandomForestClassifier(max_depth=7, random_state=0))])\n",
      "Train set performance: 0.0788\n",
      "Test set performance: 0.0811\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "print(rf_model.best_params_)\n",
    "print(rf_model.best_score_)\n",
    "print(rf_model.best_estimator_)\n",
    "\n",
    "#performance\n",
    "print('Train set performance: ' + str(round(mean_squared_error(y_train,rf_model.predict(X_train)),4))) #Train set prediction and performance\n",
    "print('Test set performance: ' + str(round(mean_squared_error(y_test,rf_model.predict(X_test)),4))) #Test set prediction and performance\n",
    "\n",
    "#Predictions\n",
    "rf_y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('xgb',\n",
       "                                        XGBClassifier(base_score=None,\n",
       "                                                      booster=None,\n",
       "                                                      callbacks=None,\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      early_stopping_rounds=None,\n",
       "                                                      enable_categorical=False,\n",
       "                                                      eval_metric=None,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      grow_policy=None,\n",
       "                                                      importance_type=None,\n",
       "                                                      interaction_cons...\n",
       "                                                      learning_rate=None,\n",
       "                                                      max_bin=None,\n",
       "                                                      max_cat_to_onehot=None,\n",
       "                                                      max_delta_step=None,\n",
       "                                                      max_depth=5,\n",
       "                                                      max_leaves=None,\n",
       "                                                      min_child_weight=None,\n",
       "                                                      missing=nan,\n",
       "                                                      monotone_constraints=None,\n",
       "                                                      n_estimators=100,\n",
       "                                                      n_jobs=None,\n",
       "                                                      num_parallel_tree=None,\n",
       "                                                      predictor=None,\n",
       "                                                      random_state=0,\n",
       "                                                      reg_alpha=None,\n",
       "                                                      reg_lambda=None, ...))]),\n",
       "             param_grid={'xgb__max_depth': [3, 5, 7],\n",
       "                         'xgb__n_estimators': [100, 200, 300]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGBoost\n",
    "xgb = xgb.XGBClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "pipe = Pipeline([('scaler', scaler),('xgb', xgb)])\n",
    "param_grid = {'xgb__max_depth': [3, 5, 7], 'xgb__n_estimators': [100, 200, 300]}\n",
    "xgb_model = GridSearchCV(pipe, param_grid, cv=5)\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "0.921675525046604\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('xgb',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "                               importance_type=None, interaction_constraints='',\n",
      "                               learning_rate=0.300000012, max_bin=256,\n",
      "                               max_cat_to_onehot=4, max_delta_step=0,\n",
      "                               max_depth=3, max_leaves=0, min_child_weight=1,\n",
      "                               missing=nan, monotone_constraints='()',\n",
      "                               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
      "                               predictor='auto', random_state=0, reg_alpha=0,\n",
      "                               reg_lambda=1, ...))])\n",
      "Train set performance: 0.0731\n",
      "Test set performance: 0.0796\n"
     ]
    }
   ],
   "source": [
    "#XGBoost\n",
    "print(xgb_model.best_params_)\n",
    "print(xgb_model.best_score_)\n",
    "print(xgb_model.best_estimator_)\n",
    "\n",
    "#performance\n",
    "print('Train set performance: ' + str(round(mean_squared_error(y_train,xgb_model.predict(X_train)),4))) #Train set prediction and performance\n",
    "print('Test set performance: ' + str(round(mean_squared_error(y_test,xgb_model.predict(X_test)),4))) #Test set prediction and performance\n",
    "\n",
    "#Predictions\n",
    "xgb_y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Neural Network Model Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Concept Drift models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Multiflow\n",
    "\n",
    "https://scikit-multiflow.readthedocs.io/en/stable/api/api.html#module-skmultiflow.drift_detection\n",
    "\n",
    "In this Library, detection methods include:\n",
    "+ ADWIN - Adaptive Windowing\n",
    "+ DDM - Drift Detection Method\n",
    "+ EDDM - Early Drift Detection Method\n",
    "+ HDDM_A - Drift Detection Method based on Hoeffding's bounds with moving average-test\n",
    "+ HDDM_W - Drift Detection Method based on Hoeffding's bounds with moving weighted average-test\n",
    "+ KSWIN - Kolmogorov-Smirnov Windowing method for concept drift detection\n",
    "+ PageHinkley - Page-Hinkley method for concept drift detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add_element(self, value)[source]\n",
    "    Add a new element to the sample window.\n",
    "    \n",
    "    Apart from adding the element value to the window, by inserting it in the correct bucket, it will also update the relevant statistics, in this case the total sum of all values, the window width and the total variance.\n",
    "    \n",
    "    Parameters\n",
    "    value: int or float (a numeric value)\n",
    "    Notes\n",
    "    \n",
    "    The value parameter can be any numeric value relevant to the analysis of concept change. For the learners in this framework we are using either 0’s or 1’s, that are interpreted as follows: 0: Means the learners prediction was wrong 1: Means the learners prediction was correct\n",
    "    \n",
    "    This function should be used at every new sample analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating lists of elements for each model with 0 meaning the learner prediction was wrong and 1 meaning the learner predictions was correct\n",
    "#These lists will be fed into the concept drift detection algorithms\n",
    "logistic_predicted_stream = []\n",
    "for i in range(len(logistic_y_pred)):\n",
    "    if logistic_y_pred[i] == y_test.values.ravel()[i]:\n",
    "        logistic_predicted_stream.append(1)\n",
    "    else:\n",
    "        logistic_predicted_stream.append(0)\n",
    "\n",
    "svm_predicted_stream = []\n",
    "for i in range(len(svm_y_pred)):\n",
    "    if svm_y_pred[i] == y_test.values.ravel()[i]:\n",
    "        svm_predicted_stream.append(1)\n",
    "    else:\n",
    "        svm_predicted_stream.append(0)\n",
    "\n",
    "rf_predicted_stream = []\n",
    "for i in range(len(rf_y_pred)):\n",
    "    if rf_y_pred[i] == y_test.values.ravel()[i]:\n",
    "        rf_predicted_stream.append(1)\n",
    "    else:\n",
    "        rf_predicted_stream.append(0)\n",
    "\n",
    "xgb_predicted_stream = []\n",
    "for i in range(len(xgb_y_pred)):\n",
    "    if xgb_y_pred[i] == y_test.values.ravel()[i]:\n",
    "        xgb_predicted_stream.append(1)\n",
    "    else:\n",
    "        xgb_predicted_stream.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change detected in data: 0 - at index: 330 for model: Support Vector Machines\n",
      "Change detected in data: 0 - at index: 362 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 426 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 522 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 682 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 778 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 874 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 1162 for model: Support Vector Machines\n",
      "Change detected in data: 0 - at index: 2698 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 725 for model: Random Forest\n",
      "Change detected in data: 1 - at index: 757 for model: Random Forest\n",
      "Change detected in data: 1 - at index: 789 for model: Random Forest\n",
      "Change detected in data: 1 - at index: 949 for model: Random Forest\n",
      "Change detected in data: 1 - at index: 1109 for model: Random Forest\n",
      "Change detected in data: 1 - at index: 1205 for model: Random Forest\n",
      "Change detected in data: 1 - at index: 1237 for model: Random Forest\n",
      "Change detected in data: 1 - at index: 1461 for model: Random Forest\n",
      "Change detected in data: 1 - at index: 1621 for model: Random Forest\n",
      "Change detected in data: 1 - at index: 2261 for model: Random Forest\n",
      "Change detected in data: 1 - at index: 2773 for model: Random Forest\n"
     ]
    }
   ],
   "source": [
    "#Adwin\n",
    "adwin = ADWIN()\n",
    "\n",
    "for i in range(len(logistic_predicted_stream)):\n",
    "    adwin.add_element(logistic_predicted_stream[i])\n",
    "    if adwin.detected_change():\n",
    "        print(\"Change detected in data: \" + str(logistic_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Logistic Regression')\n",
    "\n",
    "for i in range(len(svm_predicted_stream)):\n",
    "    adwin.add_element(svm_predicted_stream[i])\n",
    "    if adwin.detected_change():\n",
    "        adwin.get_change()\n",
    "        print(\"Change detected in data: \" + str(svm_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Support Vector Machines')\n",
    "\n",
    "for i in range(len(rf_predicted_stream)):\n",
    "    adwin.add_element(rf_predicted_stream[i])\n",
    "    if adwin.detected_change():\n",
    "        print(\"Change detected in data: \" + str(rf_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Random Forest')\n",
    "\n",
    "for i in range(len(xgb_predicted_stream)):\n",
    "    adwin.add_element(xgb_predicted_stream[i])\n",
    "    if adwin.detected_change():\n",
    "        print(\"Change detected in data: \" + str(xgb_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change detected in data: 1 - at index: 1256 for model: Random Forest\n"
     ]
    }
   ],
   "source": [
    "#DDM\n",
    "ddm = DDM()\n",
    "\n",
    "for i in range(len(logistic_predicted_stream)):\n",
    "    ddm.add_element(logistic_predicted_stream[i])\n",
    "    if ddm.detected_change():\n",
    "        print(\"Change detected in data: \" + str(logistic_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Logistic Regression')\n",
    "\n",
    "for i in range(len(svm_predicted_stream)):\n",
    "    ddm.add_element(svm_predicted_stream[i])\n",
    "    if ddm.detected_change():\n",
    "        print(\"Change detected in data: \" + str(svm_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Support Vector Machines')\n",
    "\n",
    "for i in range(len(rf_predicted_stream)):\n",
    "    ddm.add_element(rf_predicted_stream[i])\n",
    "    if ddm.detected_change():\n",
    "        print(\"Change detected in data: \" + str(rf_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Random Forest')\n",
    "\n",
    "for i in range(len(xgb_predicted_stream)):\n",
    "    ddm.add_element(xgb_predicted_stream[i])\n",
    "    if ddm.detected_change():\n",
    "        print(\"Change detected in data: \" + str(xgb_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change detected in data: 1 - at index: 68 for model: Logistic Regression\n",
      "Change detected in data: 1 - at index: 914 for model: XGBoost\n"
     ]
    }
   ],
   "source": [
    "#EDDM\n",
    "eddm = EDDM()\n",
    "\n",
    "for i in range(len(logistic_predicted_stream)):\n",
    "    eddm.add_element(logistic_predicted_stream[i])\n",
    "    if eddm.detected_change():\n",
    "        print(\"Change detected in data: \" + str(logistic_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Logistic Regression')\n",
    "\n",
    "for i in range(len(svm_predicted_stream)):\n",
    "    eddm.add_element(svm_predicted_stream[i])\n",
    "    if eddm.detected_change():\n",
    "        print(\"Change detected in data: \" + str(svm_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Support Vector Machines')\n",
    "\n",
    "for i in range(len(rf_predicted_stream)):\n",
    "    eddm.add_element(rf_predicted_stream[i])\n",
    "    if eddm.detected_change():\n",
    "        print(\"Change detected in data: \" + str(rf_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Random Forest')\n",
    "\n",
    "for i in range(len(xgb_predicted_stream)):\n",
    "    eddm.add_element(xgb_predicted_stream[i])\n",
    "    if eddm.detected_change():\n",
    "        print(\"Change detected in data: \" + str(xgb_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change detected in data: 1 - at index: 59 for model: Random Forest\n"
     ]
    }
   ],
   "source": [
    "#HDDM_A\n",
    "hddm_a = HDDM_A()\n",
    "\n",
    "for i in range(len(logistic_predicted_stream)):\n",
    "    hddm_a.add_element(logistic_predicted_stream[i])\n",
    "    if hddm_a.detected_change():\n",
    "        print(\"Change detected in data: \" + str(logistic_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Logistic Regression')\n",
    "\n",
    "for i in range(len(svm_predicted_stream)):\n",
    "    hddm_a.add_element(svm_predicted_stream[i])\n",
    "    if hddm_a.detected_change():\n",
    "        print(\"Change detected in data: \" + str(svm_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Support Vector Machines')\n",
    "\n",
    "for i in range(len(rf_predicted_stream)):\n",
    "    hddm_a.add_element(rf_predicted_stream[i])\n",
    "    if hddm_a.detected_change():\n",
    "        print(\"Change detected in data: \" + str(rf_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Random Forest')\n",
    "\n",
    "for i in range(len(xgb_predicted_stream)):\n",
    "    hddm_a.add_element(xgb_predicted_stream[i])\n",
    "    if hddm_a.detected_change():\n",
    "        print(\"Change detected in data: \" + str(xgb_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change detected in data: 1 - at index: 726 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 7373 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 11727 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 14282 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 15281 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 17592 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 19 for model: Random Forest\n"
     ]
    }
   ],
   "source": [
    "#HDDM_W\n",
    "hddm_w = HDDM_W()\n",
    "\n",
    "for i in range(len(logistic_predicted_stream)):\n",
    "    hddm_w.add_element(logistic_predicted_stream[i])\n",
    "    if hddm_w.detected_change():\n",
    "        print(\"Change detected in data: \" + str(logistic_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Logistic Regression')\n",
    "\n",
    "for i in range(len(svm_predicted_stream)):\n",
    "    hddm_w.add_element(svm_predicted_stream[i])\n",
    "    if hddm_w.detected_change():\n",
    "        print(\"Change detected in data: \" + str(svm_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Support Vector Machines')\n",
    "\n",
    "for i in range(len(rf_predicted_stream)):\n",
    "    hddm_w.add_element(rf_predicted_stream[i])\n",
    "    if hddm_w.detected_change():\n",
    "        print(\"Change detected in data: \" + str(rf_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Random Forest')\n",
    "\n",
    "for i in range(len(xgb_predicted_stream)):\n",
    "    hddm_w.add_element(xgb_predicted_stream[i])\n",
    "    if hddm_w.detected_change():\n",
    "        print(\"Change detected in data: \" + str(xgb_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbrad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\stats\\stats.py:7474: RuntimeWarning: ks_2samp: Exact calculation unsuccessful. Switching to mode=asymp.\n",
      "  warnings.warn(f\"ks_2samp: Exact calculation unsuccessful. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change detected in data: 1 - at index: 3221 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 3996 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 5204 for model: Support Vector Machines\n",
      "Change detected in data: 0 - at index: 8433 for model: Support Vector Machines\n",
      "Change detected in data: 0 - at index: 11523 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 11891 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 12063 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 17230 for model: Support Vector Machines\n",
      "Change detected in data: 1 - at index: 15 for model: Random Forest\n",
      "Change detected in data: 1 - at index: 13105 for model: Random Forest\n"
     ]
    }
   ],
   "source": [
    "#KSWIN\n",
    "kswin = KSWIN()\n",
    "\n",
    "for i in range(len(logistic_predicted_stream)):\n",
    "    kswin.add_element(logistic_predicted_stream[i])\n",
    "    if kswin.detected_change():\n",
    "        print(\"Change detected in data: \" + str(logistic_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Logistic Regression')\n",
    "\n",
    "for i in range(len(svm_predicted_stream)):\n",
    "    kswin.add_element(svm_predicted_stream[i])\n",
    "    if kswin.detected_change():\n",
    "        print(\"Change detected in data: \" + str(svm_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Support Vector Machines')\n",
    "\n",
    "for i in range(len(rf_predicted_stream)):\n",
    "    kswin.add_element(rf_predicted_stream[i])\n",
    "    if kswin.detected_change():\n",
    "        print(\"Change detected in data: \" + str(rf_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Random Forest')\n",
    "\n",
    "for i in range(len(xgb_predicted_stream)):\n",
    "    kswin.add_element(xgb_predicted_stream[i])\n",
    "    if kswin.detected_change():\n",
    "        print(\"Change detected in data: \" + str(xgb_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change detected in data: 1 - at index: 346 for model: Random Forest\n"
     ]
    }
   ],
   "source": [
    "#Page Hinkley\n",
    "page_hinkley = PageHinkley()\n",
    "\n",
    "for i in range(len(logistic_predicted_stream)):\n",
    "    page_hinkley.add_element(logistic_predicted_stream[i])\n",
    "    if page_hinkley.detected_change():\n",
    "        print(\"Change detected in data: \" + str(logistic_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Logistic Regression')\n",
    "\n",
    "for i in range(len(svm_predicted_stream)):\n",
    "    page_hinkley.add_element(svm_predicted_stream[i])\n",
    "    if page_hinkley.detected_change():\n",
    "        print(\"Change detected in data: \" + str(svm_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Support Vector Machines')\n",
    "\n",
    "for i in range(len(rf_predicted_stream)):\n",
    "    page_hinkley.add_element(rf_predicted_stream[i])\n",
    "    if page_hinkley.detected_change():\n",
    "        print(\"Change detected in data: \" + str(rf_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: Random Forest')\n",
    "\n",
    "for i in range(len(xgb_predicted_stream)):\n",
    "    page_hinkley.add_element(xgb_predicted_stream[i])\n",
    "    if page_hinkley.detected_change():\n",
    "        print(\"Change detected in data: \" + str(xgb_predicted_stream[i]) + ' - at index: ' + str(i) + ' for model: XGBoost')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a283baea478b700b62059e036c71728797acce8d35a972cf7896d5e174175201"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
